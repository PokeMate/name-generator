{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pokémon Name Generation with Keras\n",
    "\n",
    "Generate new unique Pokémon names with a LSTM using Andrej Karpathy's famous [Char-RNN](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) which he used to generate poetry. There are more information in the blog, but the concept is fairly simple. We want the build a next-character-in-text predictor. We will do this by using a window of fixed length as our input and the next char as output and then train a LSTM to perform this task. Since the network won't understand raw characters we need to encode each character to a character vectors with one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import time\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "import numpy as np\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_length = 1    # The step length we take to get our samples from our corpus\n",
    "epochs = 100       # Number of times we train on our full data\n",
    "batch_size = 32    # Data samples in each training step\n",
    "latent_dim = 64    # Size of our LSTM\n",
    "dropout_rate = 0.2 # Regularization with dropout\n",
    "model_path = os.path.realpath('./model.h5') # Location for the model\n",
    "load_model = False # Enable loading model from disk\n",
    "store_model = True # Store model to disk after training\n",
    "verbosity = 1      # Print result for each epoch\n",
    "gen_amount = 10    # How many "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "I have made a .txt where I have stored the names of Pokémon as rows. I have also done some ealy preprocessing like removing special characters and only using lowercase characters. To generate other things than Pokémon names the rows in this file can simply be replaced with some other text that one wishes to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Pokénames from file:\n",
      "bulbasaur\n",
      "chikorita\n",
      "treecko\n",
      "turtwig\n",
      "victini\n",
      "chespin\n",
      "rowlet\n",
      "ivysaur\n",
      "bayleef\n",
      "grovyle\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "input_path = os.path.realpath('./input/names.txt')\n",
    "\n",
    "input_names = []\n",
    "\n",
    "print('Reading Pokénames from file:')\n",
    "with open(input_path) as f:\n",
    "    for name in f:\n",
    "        name = name.rstrip()\n",
    "        if len(input_names) < 10:\n",
    "            print(name)\n",
    "        input_names.append(name)\n",
    "    print('...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "- Concatenate all Pokémon names into a long string corpus.\n",
    "- Build dicionaries to translate chars to indices in a binary char vector.\n",
    "- Find a suitable sequence window, I base it on the longest name I find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chars: 27\n",
      "Corpus length: 6734\n",
      "Number of names:  799\n",
      "Longest name:  12\n"
     ]
    }
   ],
   "source": [
    "# Make it all to a long string\n",
    "concat_names = '\\n'.join(input_names).lower()\n",
    "\n",
    "# Find all unique characters by using set()\n",
    "chars = sorted(list(set(concat_names)))\n",
    "num_chars = len(chars)\n",
    "\n",
    "# Build translation dictionaries, 'a' -> 0, 0 -> 'a'\n",
    "char2idx = dict((c, i) for i, c in enumerate(chars))\n",
    "idx2char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# Use longest name length as our sequence window\n",
    "max_sequence_length = max([len(name) for name in input_names])\n",
    "\n",
    "print('Total chars: {}'.format(num_chars))\n",
    "print('Corpus length:', len(concat_names))\n",
    "print('Number of names: ', len(input_names))\n",
    "print('Longest name: ', max_sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a training set where we take samples with sequence length as our input and the next char as label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 6722\n",
      "First 10 sequences and next chars:\n",
      "X=[bulbasaur ch]   y=[i]\n",
      "X=[ulbasaur chi]   y=[k]\n",
      "X=[lbasaur chik]   y=[o]\n",
      "X=[basaur chiko]   y=[r]\n",
      "X=[asaur chikor]   y=[i]\n",
      "X=[saur chikori]   y=[t]\n",
      "X=[aur chikorit]   y=[a]\n",
      "X=[ur chikorita]   y=[ ]\n",
      "X=[r chikorita ]   y=[t]\n",
      "X=[ chikorita t]   y=[r]\n"
     ]
    }
   ],
   "source": [
    "sequences = []\n",
    "next_chars = []\n",
    "\n",
    "# Loop over our data and extract pairs of sequances and next chars\n",
    "for i in range(0, len(concat_names) - max_sequence_length, step_length):\n",
    "    sequences.append(concat_names[i: i + max_sequence_length])\n",
    "    next_chars.append(concat_names[i + max_sequence_length])\n",
    "\n",
    "num_sequences = len(sequences)\n",
    "\n",
    "print('Number of sequences:', num_sequences)\n",
    "print('First 10 sequences and next chars:')\n",
    "for i in range(10):\n",
    "    print('X=[{}]   y=[{}]'.replace('\\n', ' ').format(sequences[i], next_chars[i]).replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding our data into char vectors by using the translation dictionary from earlier.\n",
    "\n",
    "#### Example\n",
    "\n",
    "- 'a'   => [1, 0, 0, ..., 0]\n",
    "\n",
    "- 'b'   => [0, 1, 0, ..., 0]\n",
    "\n",
    "- 'c'   => [0, 0, 1, ..., 0]\n",
    "\n",
    "- 'abc' => [[1, 0, 0, ..., 0], [0, 1, 0, ..., 0], [0, 0, 1, ..., 0]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (6722, 12, 27)\n",
      "Y shape: (6722, 27)\n"
     ]
    }
   ],
   "source": [
    "X = np.zeros((num_sequences, max_sequence_length, num_chars), dtype=np.bool)\n",
    "Y = np.zeros((num_sequences, num_chars), dtype=np.bool)\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for j, char in enumerate(sequence):\n",
    "        X[i, j, char2idx[char]] = 1\n",
    "    Y[i, char2idx[next_chars[i]]] = 1\n",
    "    \n",
    "print('X shape: {}'.format(X.shape))\n",
    "print('Y shape: {}'.format(Y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model\n",
    "\n",
    "Build a standard LSTM network with: \n",
    "\n",
    "- Input shape: (max_sequence_length x num_chars) - representing our sequences.\n",
    "- Output shape: num_chars - representing the next char coming after each sequence.\n",
    "- Output activation: Softmax - since only one value should be 1 in output char vector.\n",
    "- Loss: Categorical cross-entrophy - standard loss for multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 64)                23552     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 27)                1755      \n",
      "=================================================================\n",
      "Total params: 25,307\n",
      "Trainable params: 25,307\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(latent_dim, \n",
    "               input_shape=(max_sequence_length, num_chars),  \n",
    "               recurrent_dropout=dropout_rate))\n",
    "model.add(Dense(units=num_chars, activation='softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Watching the loss, doing cross-validation and all that good stuff is not that important here. The best model will not be found by optimizing some metric. We just want to strike a balance between a model that just output gibberish like 'sadsdaddddd' and model that memorizes the names it was trained on. For this it is better to just inspect the output and judge from that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training for 100 epochs\n",
      "Epoch 1/100\n",
      "6722/6722 [==============================] - 2s 236us/step - loss: 2.8115\n",
      "Epoch 2/100\n",
      "6722/6722 [==============================] - 1s 166us/step - loss: 2.5474\n",
      "Epoch 3/100\n",
      "6722/6722 [==============================] - 1s 143us/step - loss: 2.4659\n",
      "Epoch 4/100\n",
      "6722/6722 [==============================] - 1s 131us/step - loss: 2.4084\n",
      "Epoch 5/100\n",
      "6722/6722 [==============================] - 1s 140us/step - loss: 2.3534\n",
      "Epoch 6/100\n",
      "6722/6722 [==============================] - 1s 133us/step - loss: 2.3006\n",
      "Epoch 7/100\n",
      "6722/6722 [==============================] - 1s 142us/step - loss: 2.2499\n",
      "Epoch 8/100\n",
      "6722/6722 [==============================] - 1s 132us/step - loss: 2.2055\n",
      "Epoch 9/100\n",
      "6722/6722 [==============================] - 1s 125us/step - loss: 2.1553\n",
      "Epoch 10/100\n",
      "6722/6722 [==============================] - 1s 123us/step - loss: 2.1197\n",
      "Epoch 11/100\n",
      "6722/6722 [==============================] - 1s 123us/step - loss: 2.0794\n",
      "Epoch 12/100\n",
      "6722/6722 [==============================] - 1s 124us/step - loss: 2.0356\n",
      "Epoch 13/100\n",
      "6722/6722 [==============================] - 1s 125us/step - loss: 2.0054\n",
      "Epoch 14/100\n",
      "6722/6722 [==============================] - 1s 135us/step - loss: 1.9799\n",
      "Epoch 15/100\n",
      "6722/6722 [==============================] - 1s 126us/step - loss: 1.9553\n",
      "Epoch 16/100\n",
      "6722/6722 [==============================] - 1s 127us/step - loss: 1.9251\n",
      "Epoch 17/100\n",
      "6722/6722 [==============================] - 1s 126us/step - loss: 1.9167\n",
      "Epoch 18/100\n",
      "6722/6722 [==============================] - 1s 127us/step - loss: 1.8969\n",
      "Epoch 19/100\n",
      "6722/6722 [==============================] - 1s 125us/step - loss: 1.8748\n",
      "Epoch 20/100\n",
      "6722/6722 [==============================] - 1s 127us/step - loss: 1.8462\n",
      "Epoch 21/100\n",
      "6722/6722 [==============================] - 1s 129us/step - loss: 1.8393\n",
      "Epoch 22/100\n",
      "6722/6722 [==============================] - 1s 129us/step - loss: 1.8320\n",
      "Epoch 23/100\n",
      "6722/6722 [==============================] - 1s 137us/step - loss: 1.8079\n",
      "Epoch 24/100\n",
      "6722/6722 [==============================] - 1s 135us/step - loss: 1.7999\n",
      "Epoch 25/100\n",
      "6722/6722 [==============================] - 1s 133us/step - loss: 1.7849\n",
      "Epoch 26/100\n",
      "6722/6722 [==============================] - 1s 136us/step - loss: 1.7664\n",
      "Epoch 27/100\n",
      "6722/6722 [==============================] - 1s 133us/step - loss: 1.7450\n",
      "Epoch 28/100\n",
      "6722/6722 [==============================] - 1s 138us/step - loss: 1.7395\n",
      "Epoch 29/100\n",
      "6722/6722 [==============================] - 1s 137us/step - loss: 1.7309\n",
      "Epoch 30/100\n",
      "6722/6722 [==============================] - 1s 137us/step - loss: 1.7131\n",
      "Epoch 31/100\n",
      "6722/6722 [==============================] - 1s 133us/step - loss: 1.7059\n",
      "Epoch 32/100\n",
      "6722/6722 [==============================] - 1s 139us/step - loss: 1.6971\n",
      "Epoch 33/100\n",
      "6722/6722 [==============================] - 1s 144us/step - loss: 1.6896\n",
      "Epoch 34/100\n",
      "6722/6722 [==============================] - 1s 140us/step - loss: 1.6752\n",
      "Epoch 35/100\n",
      "6722/6722 [==============================] - 1s 151us/step - loss: 1.6648\n",
      "Epoch 36/100\n",
      "6722/6722 [==============================] - 1s 148us/step - loss: 1.6393\n",
      "Epoch 37/100\n",
      "6722/6722 [==============================] - 1s 128us/step - loss: 1.6419\n",
      "Epoch 38/100\n",
      "6722/6722 [==============================] - 1s 127us/step - loss: 1.6389\n",
      "Epoch 39/100\n",
      "6722/6722 [==============================] - 1s 125us/step - loss: 1.6176\n",
      "Epoch 40/100\n",
      "6722/6722 [==============================] - 1s 129us/step - loss: 1.6127\n",
      "Epoch 41/100\n",
      "6722/6722 [==============================] - 1s 135us/step - loss: 1.6028\n",
      "Epoch 42/100\n",
      "6722/6722 [==============================] - 1s 125us/step - loss: 1.6041\n",
      "Epoch 43/100\n",
      "6722/6722 [==============================] - 1s 130us/step - loss: 1.5995\n",
      "Epoch 44/100\n",
      "6722/6722 [==============================] - 1s 137us/step - loss: 1.5903\n",
      "Epoch 45/100\n",
      "6722/6722 [==============================] - 1s 136us/step - loss: 1.5758\n",
      "Epoch 46/100\n",
      "6722/6722 [==============================] - 1s 133us/step - loss: 1.5748\n",
      "Epoch 47/100\n",
      "6722/6722 [==============================] - 1s 130us/step - loss: 1.5668\n",
      "Epoch 48/100\n",
      "6722/6722 [==============================] - 1s 136us/step - loss: 1.5443\n",
      "Epoch 49/100\n",
      "6722/6722 [==============================] - 1s 130us/step - loss: 1.5638\n",
      "Epoch 50/100\n",
      "6722/6722 [==============================] - 1s 134us/step - loss: 1.5295\n",
      "Epoch 51/100\n",
      "6722/6722 [==============================] - 1s 128us/step - loss: 1.5372\n",
      "Epoch 52/100\n",
      "6722/6722 [==============================] - 1s 133us/step - loss: 1.5123\n",
      "Epoch 53/100\n",
      "6722/6722 [==============================] - 1s 135us/step - loss: 1.5127\n",
      "Epoch 54/100\n",
      "6722/6722 [==============================] - 1s 135us/step - loss: 1.5237\n",
      "Epoch 55/100\n",
      "6722/6722 [==============================] - 1s 129us/step - loss: 1.5042\n",
      "Epoch 56/100\n",
      "6722/6722 [==============================] - 1s 126us/step - loss: 1.5072\n",
      "Epoch 57/100\n",
      "6722/6722 [==============================] - 1s 127us/step - loss: 1.4939\n",
      "Epoch 58/100\n",
      "6722/6722 [==============================] - 1s 128us/step - loss: 1.4894\n",
      "Epoch 59/100\n",
      "6722/6722 [==============================] - 1s 126us/step - loss: 1.4884\n",
      "Epoch 60/100\n",
      "6722/6722 [==============================] - 1s 126us/step - loss: 1.4764\n",
      "Epoch 61/100\n",
      "6722/6722 [==============================] - 1s 125us/step - loss: 1.4762\n",
      "Epoch 62/100\n",
      "6722/6722 [==============================] - 1s 126us/step - loss: 1.4735\n",
      "Epoch 63/100\n",
      "6722/6722 [==============================] - 1s 126us/step - loss: 1.4608\n",
      "Epoch 64/100\n",
      "6722/6722 [==============================] - 1s 128us/step - loss: 1.4731\n",
      "Epoch 65/100\n",
      "6722/6722 [==============================] - 1s 126us/step - loss: 1.4639\n",
      "Epoch 66/100\n",
      "6722/6722 [==============================] - 1s 133us/step - loss: 1.4665\n",
      "Epoch 67/100\n",
      "6722/6722 [==============================] - 1s 143us/step - loss: 1.4631\n",
      "Epoch 68/100\n",
      "6722/6722 [==============================] - 1s 139us/step - loss: 1.4399\n",
      "Epoch 69/100\n",
      "6722/6722 [==============================] - 1s 133us/step - loss: 1.4565\n",
      "Epoch 70/100\n",
      "6722/6722 [==============================] - 1s 150us/step - loss: 1.4307\n",
      "Epoch 71/100\n",
      "6722/6722 [==============================] - 1s 149us/step - loss: 1.4173\n",
      "Epoch 72/100\n",
      "6722/6722 [==============================] - 1s 148us/step - loss: 1.4418\n",
      "Epoch 73/100\n",
      "6722/6722 [==============================] - 1s 139us/step - loss: 1.4201\n",
      "Epoch 74/100\n",
      "6722/6722 [==============================] - 1s 125us/step - loss: 1.4180\n",
      "Epoch 75/100\n",
      "6722/6722 [==============================] - 1s 138us/step - loss: 1.4190\n",
      "Epoch 76/100\n",
      "6722/6722 [==============================] - 1s 149us/step - loss: 1.4087\n",
      "Epoch 77/100\n",
      "6722/6722 [==============================] - 1s 160us/step - loss: 1.4200\n",
      "Epoch 78/100\n",
      "6722/6722 [==============================] - 1s 145us/step - loss: 1.4151\n",
      "Epoch 79/100\n",
      "6722/6722 [==============================] - 1s 129us/step - loss: 1.4064\n",
      "Epoch 80/100\n",
      "6722/6722 [==============================] - 1s 147us/step - loss: 1.4057\n",
      "Epoch 81/100\n",
      "6722/6722 [==============================] - 1s 149us/step - loss: 1.3838\n",
      "Epoch 82/100\n",
      "6722/6722 [==============================] - 1s 144us/step - loss: 1.3875\n",
      "Epoch 83/100\n",
      "6722/6722 [==============================] - 1s 135us/step - loss: 1.3678\n",
      "Epoch 84/100\n",
      "6722/6722 [==============================] - 1s 134us/step - loss: 1.3856\n",
      "Epoch 85/100\n",
      "6722/6722 [==============================] - 1s 135us/step - loss: 1.3788\n",
      "Epoch 86/100\n",
      "6722/6722 [==============================] - 1s 133us/step - loss: 1.3790\n",
      "Epoch 87/100\n",
      "6722/6722 [==============================] - 1s 136us/step - loss: 1.3956\n",
      "Epoch 88/100\n",
      "6722/6722 [==============================] - 1s 134us/step - loss: 1.3906\n",
      "Epoch 89/100\n",
      "6722/6722 [==============================] - 1s 135us/step - loss: 1.3724\n",
      "Epoch 90/100\n",
      "6722/6722 [==============================] - 1s 133us/step - loss: 1.3761\n",
      "Epoch 91/100\n",
      "6722/6722 [==============================] - 1s 132us/step - loss: 1.3672\n",
      "Epoch 92/100\n",
      "6722/6722 [==============================] - 1s 130us/step - loss: 1.3695\n",
      "Epoch 93/100\n",
      "6722/6722 [==============================] - 1s 136us/step - loss: 1.3717\n",
      "Epoch 94/100\n",
      "6722/6722 [==============================] - 1s 132us/step - loss: 1.3723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/100\n",
      "6722/6722 [==============================] - 1s 127us/step - loss: 1.3569\n",
      "Epoch 96/100\n",
      "6722/6722 [==============================] - 1s 133us/step - loss: 1.3578\n",
      "Epoch 97/100\n",
      "6722/6722 [==============================] - 1s 133us/step - loss: 1.3543\n",
      "Epoch 98/100\n",
      "6722/6722 [==============================] - 1s 129us/step - loss: 1.3494\n",
      "Epoch 99/100\n",
      "6722/6722 [==============================] - 1s 127us/step - loss: 1.3535\n",
      "Epoch 100/100\n",
      "6722/6722 [==============================] - 1s 129us/step - loss: 1.3539\n",
      "Finished training - time elapsed: 1.5189442833264668 min\n",
      "Storing model at: /Users/simonbachmann/Programming/pokemate/name-generator/training/model.h5\n"
     ]
    }
   ],
   "source": [
    "if load_model:\n",
    "    model.load_weights(model_path)\n",
    "else:\n",
    "    \n",
    "    start = time.time()\n",
    "    print('Start training for {} epochs'.format(epochs))\n",
    "    history = model.fit(X, Y, epochs=epochs, batch_size=batch_size, verbose=verbosity)\n",
    "    end = time.time()\n",
    "    print('Finished training - time elapsed:', (end - start)/60, 'min')\n",
    "    \n",
    "if store_model:\n",
    "    print('Storing model at:', model_path)\n",
    "    model.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation\n",
    "\n",
    "Generate names by starting with a real sequence from the corpus, continuously predicting the next char while updating the sequence. To get diversity the correct char is selected from a probability distribution based on the models prediction. This can also be furthered by something called temperature, which I didn't use here.\n",
    "\n",
    "I also added some postprocessing to remove things I did not like manually. Some of this could possibly be done by teaking the network, but I was happy with the way the names looked overall. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 new names are being generated\n",
      "Generated 1\n",
      "Generated 2\n",
      "Generated 3\n",
      "Generated 4\n",
      "Generated 5\n",
      "Generated 6\n",
      "Generated 7\n",
      "Generated 8\n",
      "Generated 9\n",
      "Generated 10\n"
     ]
    }
   ],
   "source": [
    "# Start sequence generation from end of the input sequence\n",
    "sequence = concat_names[-(max_sequence_length - 1):] + '\\n'\n",
    "\n",
    "new_names = []\n",
    "\n",
    "print('{} new names are being generated'.format(gen_amount))\n",
    "\n",
    "while len(new_names) < gen_amount:\n",
    "    \n",
    "    # Vectorize sequence for prediction\n",
    "    x = np.zeros((1, max_sequence_length, num_chars))\n",
    "    for i, char in enumerate(sequence):\n",
    "        x[0, i, char2idx[char]] = 1\n",
    "\n",
    "    # Sample next char from predicted probabilities\n",
    "    probs = model.predict(x, verbose=0)[0]\n",
    "    probs /= probs.sum()\n",
    "    next_idx = np.random.choice(len(probs), p=probs)   \n",
    "    next_char = idx2char[next_idx]   \n",
    "    sequence = sequence[1:] + next_char\n",
    "\n",
    "    # New line means we have a new name\n",
    "    if next_char == '\\n':\n",
    "\n",
    "        gen_name = [name for name in sequence.split('\\n')][1]\n",
    "\n",
    "        # Never start name with two identical chars, could probably also\n",
    "        if len(gen_name) > 2 and gen_name[0] == gen_name[1]:\n",
    "            gen_name = gen_name[1:]\n",
    "\n",
    "        # Discard all names that are too short\n",
    "        if len(gen_name) > 2:\n",
    "            \n",
    "            # Only allow new and unique names\n",
    "            if gen_name not in input_names + new_names:\n",
    "                new_names.append(gen_name.capitalize())\n",
    "\n",
    "        if 0 == (len(new_names) % (gen_amount/ 10)):\n",
    "            print('Generated {}'.format(len(new_names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Here are the results. I personally cannot tell the difference between generated names and names of Pokémon I dont know. Sometimes there are giveaways, but overall the names are convincing and diverse!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 generated names:\n",
      "Vinilita\n",
      "Goltar\n",
      "Fooby\n",
      "Slavali\n",
      "Douffa\n",
      "Fernoon\n",
      "Reiopic\n",
      "Terriclue\n",
      "Houana\n",
      "Dible\n"
     ]
    }
   ],
   "source": [
    "print_first_n = min(10, gen_amount)\n",
    "\n",
    "print('First {} generated names:'.format(print_first_n))\n",
    "for name in new_names[:print_first_n]:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_output = '\\n'.join(sorted(new_names))\n",
    "output_path = os.path.realpath('./output/generated_names.txt')\n",
    "\n",
    "with open(output_path, 'w') as f:\n",
    "    f.write(concat_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
